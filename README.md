# Light_Transformer

## Introduction

<br>


## Model Architecture

<br>

### Vanilla Transformer

<br>

### Light Transformer


<br>

<br>

## Training Setup

### In common
* Dataset: downsized WMT14 (EN-DE)
* Tokenization: BPE
* Batch_size: 128
* Num of Epochs:
* Learning Rate:
* 



### Only on Vanilla Transformer

<br>

### Only on Light Transformer


<br>


## Results
Evaluation
Speed

<br>

<br>

## Reference
Universal Transformer
Transformer XL
